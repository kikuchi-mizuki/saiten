"""
å£°ç´‹ç®¡ç†ãƒ»éŸ³å£°è­˜åˆ¥ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
"""

from fastapi import APIRouter, HTTPException, Depends, UploadFile, File
from pydantic import BaseModel
from typing import Dict, List, Optional
import tempfile
import os
import numpy as np
from datetime import datetime

from utils.voiceprint_extractor import get_voiceprint_extractor
from utils.speaker_diarization import get_speaker_diarization

# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ main.py ã‹ã‚‰ import ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®š
router = APIRouter(prefix="/voiceprint", tags=["voiceprint"])


class VoiceprintCreate(BaseModel):
	"""å£°ç´‹ç™»éŒ²ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
	voiceprint_name: str
	audio_duration_seconds: int
	embedding: List[float]
	confidence_score: Optional[float] = None
	metadata: Optional[Dict] = None


class VoiceprintResponse(BaseModel):
	"""å£°ç´‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
	id: str
	user_id: str
	voiceprint_name: str
	audio_duration_seconds: int
	sample_count: int
	confidence_score: Optional[float]
	metadata: Optional[Dict]
	created_at: str
	updated_at: str
	is_active: bool


@router.post("/register")
async def register_voiceprint(
	file: UploadFile = File(...),
	voiceprint_name: str = "",
	supabase=None,
	user: dict = None
):
	"""
	æ•™æˆã®å£°ç´‹ã‚’ç™»éŒ²

	éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å£°ç´‹ã‚’æŠ½å‡ºã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚
	åˆå›ç™»éŒ²å¾Œã€ä»¥é™ã®éŸ³å£°ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§è‡ªå‹•è­˜åˆ¥ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
	"""
	if not supabase:
		raise HTTPException(status_code=500, detail="SupabaseãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

	temp_audio_path = None

	try:
		# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
		with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp:
			content = await file.read()
			tmp.write(content)
			temp_audio_path = tmp.name

		print(f"ğŸ¤ å£°ç´‹ç™»éŒ²é–‹å§‹: {file.filename}")

		# å£°ç´‹æŠ½å‡º
		extractor = get_voiceprint_extractor()
		embedding = extractor.extract_voiceprint(temp_audio_path)
		duration = extractor.get_audio_duration(temp_audio_path)

		# å£°ç´‹åï¼ˆæœªæŒ‡å®šã®å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰
		if not voiceprint_name:
			voiceprint_name = f"æ•™æˆã®å£°_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

		# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
		voiceprint_data = {
			"user_id": user["id"],
			"voiceprint_name": voiceprint_name,
			"embedding": embedding.tolist(),
			"audio_duration_seconds": duration,
			"sample_count": 1,
			"confidence_score": 0.95,  # åˆå›ç™»éŒ²æ™‚ã¯0.95
			"metadata": {
				"filename": file.filename,
				"registered_at": datetime.now().isoformat()
			},
			"is_active": True
		}

		response = supabase.table("professor_voiceprints").insert(voiceprint_data).execute()

		print(f"âœ… å£°ç´‹ç™»éŒ²å®Œäº†: {voiceprint_name} (duration: {duration}s)")

		return {
			"success": True,
			"message": "å£°ç´‹ã‚’ç™»éŒ²ã—ã¾ã—ãŸ",
			"voiceprint_id": response.data[0]["id"],
			"voiceprint_name": voiceprint_name,
			"audio_duration_seconds": duration,
			"confidence_score": 0.95
		}

	except Exception as e:
		print(f"âŒ å£°ç´‹ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
		raise HTTPException(status_code=500, detail=f"å£°ç´‹ã®ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

	finally:
		# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
		if temp_audio_path and os.path.exists(temp_audio_path):
			os.unlink(temp_audio_path)


@router.get("/list")
async def list_voiceprints(supabase=None, user: dict = None):
	"""
	ç™»éŒ²æ¸ˆã¿å£°ç´‹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
	"""
	if not supabase:
		raise HTTPException(status_code=500, detail="SupabaseãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

	try:
		response = supabase.table("professor_voiceprints") \
			.select("*") \
			.eq("user_id", user["id"]) \
			.order("created_at", desc=True) \
			.execute()

		return {
			"success": True,
			"voiceprints": response.data
		}

	except Exception as e:
		raise HTTPException(status_code=500, detail=f"å£°ç´‹ãƒªã‚¹ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")


@router.delete("/{voiceprint_id}")
async def delete_voiceprint(voiceprint_id: str, supabase=None, user: dict = None):
	"""
	å£°ç´‹ã‚’å‰Šé™¤
	"""
	if not supabase:
		raise HTTPException(status_code=500, detail="SupabaseãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

	try:
		# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å£°ç´‹ã‹ãƒã‚§ãƒƒã‚¯
		check_response = supabase.table("professor_voiceprints") \
			.select("id") \
			.eq("id", voiceprint_id) \
			.eq("user_id", user["id"]) \
			.execute()

		if not check_response.data:
			raise HTTPException(status_code=404, detail="å£°ç´‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

		# å‰Šé™¤
		supabase.table("professor_voiceprints").delete().eq("id", voiceprint_id).execute()

		return {
			"success": True,
			"message": "å£°ç´‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
		}

	except HTTPException:
		raise
	except Exception as e:
		raise HTTPException(status_code=500, detail=f"å£°ç´‹ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")


@router.post("/identify-speakers")
async def identify_speakers_endpoint(
	file: UploadFile = File(...),
	user: dict = None
):
	"""
	éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©±è€…ã‚’è­˜åˆ¥

	pyannote.audioã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‹ã‚‰è©±è€…ã‚’è­˜åˆ¥ã—ã€
	å„è©±è€…ã®ç™ºè¨€æ™‚é–“å¸¯ã‚’ç‰¹å®šã—ã¾ã™ã€‚
	"""
	temp_audio_path = None

	try:
		# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
		with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp:
			content = await file.read()
			tmp.write(content)
			temp_audio_path = tmp.name

		print(f"ğŸ” è©±è€…è­˜åˆ¥é–‹å§‹: {file.filename}")

		# è©±è€…è­˜åˆ¥
		diarization = get_speaker_diarization()
		segments = diarization.identify_speakers(temp_audio_path)
		statistics = diarization.get_speaker_statistics(segments)

		print(f"âœ… è©±è€…è­˜åˆ¥å®Œäº†: {statistics['total_speakers']}äººæ¤œå‡º")

		return {
			"success": True,
			"filename": file.filename,
			"statistics": statistics,
			"segments": [s.to_dict() for s in segments]
		}

	except Exception as e:
		print(f"âŒ è©±è€…è­˜åˆ¥ã‚¨ãƒ©ãƒ¼: {e}")
		raise HTTPException(status_code=500, detail=f"è©±è€…è­˜åˆ¥ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

	finally:
		# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
		if temp_audio_path and os.path.exists(temp_audio_path):
			os.unlink(temp_audio_path)


@router.post("/extract-professor-speech")
async def extract_professor_speech(
	file: UploadFile = File(...),
	use_voiceprint: bool = True,
	supabase=None,
	user: dict = None
):
	"""
	éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ•™æˆã®ç™ºè¨€ã®ã¿ã‚’æŠ½å‡º

	1. è©±è€…è­˜åˆ¥ï¼ˆpyannote.audioï¼‰
	2. å„è©±è€…ã®å£°ç´‹ã‚’æŠ½å‡º
	3. ç™»éŒ²æ¸ˆã¿å£°ç´‹ã¨ç…§åˆã—ã¦æ•™æˆã‚’ç‰¹å®š
	4. Whisper APIã§æ–‡å­—èµ·ã“ã—
	5. æ•™æˆã®ç™ºè¨€ã®ã¿ã‚’è¿”ã™
	"""
	if not supabase:
		raise HTTPException(status_code=500, detail="SupabaseãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

	temp_audio_path = None

	try:
		from openai import OpenAI
		openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

		# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
		with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp:
			content = await file.read()
			tmp.write(content)
			temp_audio_path = tmp.name

		print(f"ğŸ¤ æ•™æˆéŸ³å£°æŠ½å‡ºé–‹å§‹: {file.filename}")

		# ã‚¹ãƒ†ãƒƒãƒ—1: è©±è€…è­˜åˆ¥
		print("ğŸ“Š Step 1/5: è©±è€…è­˜åˆ¥ä¸­...")
		diarization = get_speaker_diarization()
		segments = diarization.identify_speakers(temp_audio_path)
		statistics = diarization.get_speaker_statistics(segments)

		print(f"âœ… {statistics['total_speakers']}äººã®è©±è€…ã‚’æ¤œå‡º")

		# ã‚¹ãƒ†ãƒƒãƒ—2: æ•™æˆã‚’ç‰¹å®š
		print("ğŸ” Step 2/5: æ•™æˆã‚’ç‰¹å®šä¸­...")
		professor_speaker_id = None
		best_match_score = 0.0

		if use_voiceprint:
			# ç™»éŒ²æ¸ˆã¿å£°ç´‹ã¨ç…§åˆ
			voiceprints_response = supabase.table("professor_voiceprints") \
				.select("*") \
				.eq("user_id", user["id"]) \
				.eq("is_active", True) \
				.order("created_at", desc=True) \
				.limit(1) \
				.execute()

			if voiceprints_response.data:
				# ç™»éŒ²æ¸ˆã¿å£°ç´‹ãŒã‚ã‚‹å ´åˆ
				registered_voiceprint = voiceprints_response.data[0]
				registered_embedding = np.array(registered_voiceprint["embedding"])

				extractor = get_voiceprint_extractor()

				# å„è©±è€…ã®å£°ç´‹ã‚’æŠ½å‡ºã—ã¦ç…§åˆ
				best_match_speaker = None

				for speaker_info in statistics["speakers"]:
					speaker_id = speaker_info["speaker_id"]
					speaker_segments = diarization.filter_segments_by_speaker(segments, speaker_id)

					# æœ€åˆã®3ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰å£°ç´‹ã‚’æŠ½å‡ºï¼ˆä»£è¡¨çš„ãªå£°ç´‹ã‚’å–å¾—ï¼‰
					speaker_embeddings = []
					for seg in speaker_segments[:3]:
						try:
							embedding = extractor.extract_voiceprint(
								temp_audio_path,
								start_time=seg.start,
								end_time=seg.end
							)
							speaker_embeddings.append(embedding)
						except:
							continue

					if speaker_embeddings:
						# å¹³å‡å£°ç´‹ã‚’è¨ˆç®—
						avg_embedding = extractor.merge_voiceprints(speaker_embeddings)

						# ç™»éŒ²æ¸ˆã¿å£°ç´‹ã¨ç…§åˆ
						similarity = extractor.compare_voiceprints(registered_embedding, avg_embedding)

						print(f"  - {speaker_id}: é¡ä¼¼åº¦ {similarity:.2%}")

						if similarity > best_match_score:
							best_match_score = similarity
							best_match_speaker = speaker_id

				# é–¾å€¤ä»¥ä¸Šãªã‚‰æ•™æˆã¨åˆ¤å®š
				if best_match_score >= 0.75:
					professor_speaker_id = best_match_speaker
					print(f"âœ… å£°ç´‹ç…§åˆæˆåŠŸ: {professor_speaker_id} (é¡ä¼¼åº¦: {best_match_score:.2%})")
				else:
					print(f"âš ï¸ å£°ç´‹ç…§åˆå¤±æ•—: æœ€é«˜é¡ä¼¼åº¦ {best_match_score:.2%} < 75%")

		# å£°ç´‹ç…§åˆã«å¤±æ•—ã—ãŸå ´åˆã¯æœ€ã‚‚ç™ºè¨€æ™‚é–“ãŒé•·ã„è©±è€…ã‚’æ•™æˆã¨åˆ¤å®š
		if professor_speaker_id is None:
			professor_speaker_id = diarization.identify_longest_speaker(segments)
			print(f"â„¹ï¸ ç™ºè¨€æ™‚é–“ãƒ™ãƒ¼ã‚¹ã§åˆ¤å®š: {professor_speaker_id}")

		# ã‚¹ãƒ†ãƒƒãƒ—3: Whisper APIã§æ–‡å­—èµ·ã“ã—
		print("ğŸ“ Step 3/5: Whisper APIã§æ–‡å­—èµ·ã“ã—ä¸­...")
		with open(temp_audio_path, "rb") as audio_file:
			transcript = openai_client.audio.transcriptions.create(
				model="whisper-1",
				file=audio_file,
				language="ja",
				response_format="verbose_json",
				timestamp_granularities=["segment"]
			)

		whisper_segments = [
			{
				"start": seg["start"],
				"end": seg["end"],
				"text": seg["text"]
			}
			for seg in transcript.segments
		]

		print(f"âœ… æ–‡å­—èµ·ã“ã—å®Œäº†: {len(whisper_segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")

		# ã‚¹ãƒ†ãƒƒãƒ—4: è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨Whisperã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒƒãƒãƒ³ã‚°
		print("ğŸ”— Step 4/5: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒƒãƒãƒ³ã‚°ä¸­...")
		segments_with_text = diarization.match_segments_with_transcript(segments, whisper_segments)

		# ã‚¹ãƒ†ãƒƒãƒ—5: æ•™æˆã®ç™ºè¨€ã®ã¿ã‚’æŠ½å‡º
		print("âœ‚ï¸ Step 5/5: æ•™æˆã®ç™ºè¨€ã‚’æŠ½å‡ºä¸­...")
		professor_text = diarization.extract_speaker_text(segments_with_text, professor_speaker_id)

		print(f"âœ… æ•™æˆéŸ³å£°æŠ½å‡ºå®Œäº†: {len(professor_text)}æ–‡å­—")

		return {
			"success": True,
			"filename": file.filename,
			"professor_speaker_id": professor_speaker_id,
			"professor_text": professor_text,
			"text_length": len(professor_text),
			"statistics": statistics,
			"match_method": "voiceprint" if use_voiceprint and best_match_score >= 0.75 else "longest_speaker",
			"match_score": best_match_score if use_voiceprint else None
		}

	except Exception as e:
		print(f"âŒ æ•™æˆéŸ³å£°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
		import traceback
		traceback.print_exc()
		raise HTTPException(status_code=500, detail=f"æ•™æˆéŸ³å£°ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

	finally:
		# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
		if temp_audio_path and os.path.exists(temp_audio_path):
			os.unlink(temp_audio_path)
