from fastapi import FastAPI
from pydantic import BaseModel
from typing import Dict, List, Optional, Tuple
import json
import pathlib
import re
import os
import requests

ROOT = pathlib.Path(__file__).resolve().parents[1]
SAMPLE_PATH = ROOT / "data" / "sample_comments.json"
PROMPTS_DIR = ROOT / "prompts"

# --- simple .env loader (no external deps)
def _load_local_env():
	env_path = ROOT / ".env"
	if not env_path.exists():
		return
	for line in env_path.read_text(encoding="utf-8").splitlines():
		line = line.strip()
		if not line or line.startswith("#"):
			continue
		if "=" in line:
			k, v = line.split("=", 1)
			k = k.strip()
			v = v.strip().strip('"').strip("'")
			os.environ.setdefault(k, v)

_load_local_env()

app = FastAPI(title="æ•™æˆã‚³ãƒ¡ãƒ³ãƒˆè‡ªå‹•åŒ–Bot API (MVP)")


class GenerateOptions(BaseModel):
	length: Optional[int] = 400
	tone: Optional[str] = "æ¸©ã‹ã‚"


class GenerateRequest(BaseModel):
	text: str
	type: str  # 'reflection' | 'final'
	rubric: Optional[Dict[str, int]] = None
	options: Optional[GenerateOptions] = None


class GenerateResponse(BaseModel):
	ai_comment: str
	used_refs: List[str] = []
	tokens: Optional[int] = None
	latency_ms: Optional[int] = None


# è¿½åŠ : ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãƒ‰ãƒ©ãƒ•ãƒˆï¼‹Rubricã‚’è¿”ã™MVPç”¨
class DirectGenRequest(BaseModel):
	text: str
	type: Optional[str] = "reflection"


def load_samples() -> List[Dict]:
	try:
		return json.loads(SAMPLE_PATH.read_text(encoding="utf-8"))
	except Exception:
		return []


def tokenize(s: str) -> List[str]:
	return [t for t in s.replace("\n", " ").replace("ã€", " ").replace("ã€‚", " ").split() if t]


def jaccard(a: List[str], b: List[str]) -> float:
	set_a, set_b = set(a), set(b)
	if not set_a and not set_b:
		return 0.0
	return len(set_a & set_b) / max(1, len(set_a | set_b))


def retrieve_refs(text: str, doc_type: str, k: int = 2) -> List[str]:
	samples = load_samples()
	toks = tokenize(text)
	candidates = [s for s in samples if s.get("type") == ("reflection" if doc_type == "reflection" else "final")]
	scored = sorted(((jaccard(toks, tokenize(c.get("text", ""))), c.get("text", "")) for c in candidates), reverse=True)
	return [t for _, t in scored[:k]]


def load_prompt(doc_type: str) -> str:
	file = "reflection.txt" if doc_type == "reflection" else "final.txt"
	return (PROMPTS_DIR / file).read_text(encoding="utf-8")


def build_comment(req: GenerateRequest) -> GenerateResponse:
	refs = retrieve_refs(req.text, req.type, k=2)
	prefix = "â—‹ " if req.type == "reflection" else ""
	length = req.options.length if req.options else 400
	tone = req.options.tone if req.options else "æ¸©ã‹ã‚"
	rub = req.rubric or {}
	rub_text = ", ".join([f"{k}:{v}" for k, v in rub.items()]) if rub else ""

	intro = f"{prefix}å…¥åŠ›å†…å®¹ã‚’è¸ã¾ãˆã€æ•™æˆã®èªã‚Šå£ã§ä¸‹æ›¸ãã‚’æç¤ºã—ã¾ã™ã€‚"
	rationale = f"{prefix}å‚ç…§ä¾‹ã«ã‚ˆã‚Šæ–‡ä½“ã®ä¸€è²«æ€§ã‚’æ‹…ä¿ã—ã€æ¬¡ã®ä¸€æ­©ã‚’ç¤ºå”†ã—ã¾ã™ã€‚"
	cond = f"{prefix}ç›®å®‰æ–‡å­—æ•°:{length}ï¼ãƒˆãƒ¼ãƒ³:{tone}ã€‚Rubric:{rub_text}" if rub_text else f"{prefix}ç›®å®‰æ–‡å­—æ•°:{length}ï¼ãƒˆãƒ¼ãƒ³:{tone}ã€‚"

	body = "\n".join([intro, rationale, cond])
	if req.type == "final":
		body = "\n".join([
			"å…¨ä½“è©•ä¾¡: å­¦ã³ã®æ¥ç¶šã¨ä»®èª¬ã®ç­‹ãŒè‰¯ã„ã¨æ„Ÿã˜ã¾ã™ã€‚",
			"å¼·ã¿: è¦³å¯Ÿã¨æ„æ€æ±ºå®šã®ä¸€è²«æ€§ã€‚",
			"æ”¹å–„: æ’¤é€€åŸºæº–ã¨å®Ÿè£…é †ã®æ˜ç¢ºåŒ–ã€‚",
			"ç·æ‹¬: ã‚ã‚Šæ–¹ã«ç«‹è„šã—ã€æ¬¡ã®ä¸€æ­©ã‚’å…·ä½“åŒ–ã—ã¾ã—ã‚‡ã†ã€‚",
		])

	return GenerateResponse(ai_comment=body, used_refs=refs, tokens=None, latency_ms=None)


# è¿½åŠ : ç°¡æ˜“Rubricç®—å‡º
RUBRIC_CATEGORIES = ["ç†è§£åº¦", "è«–ç†æ€§", "ç‹¬è‡ªæ€§", "å®Ÿè·µæ€§", "è¡¨ç¾åŠ›"]


def simple_score(text: str) -> Dict[str, any]:
	"""Rubricã‚¹ã‚³ã‚¢ã¨ç†ç”±ã‚’ç”Ÿæˆ"""
	length = len(text)
	has_examples = any(k in text for k in ["å…·ä½“", "äº‹ä¾‹", "ä¾‹ãˆã°", "ç¾å ´", "å®Ÿè£…", "æ¤œè¨¼"])
	first_person = any(k in text for k in ["ç§ã¯", "è‡ªåˆ†", "çµŒé¨“", "å®Ÿä½“é¨“"])
	logical_markers = sum(k in text for k in ["ãªãœ", "ã—ãŸãŒã£ã¦", "ä¸€æ–¹ã§", "ã¤ã¾ã‚Š", "å‰æ", "ä»®èª¬"])
	clarity = sum(text.count(sym) for sym in ["ã€‚", "ã€", "\n"]) > 3
	
	# ã‚¹ã‚³ã‚¢è¨ˆç®—
	score_values = {
		"ç†è§£åº¦": 3 + (1 if logical_markers >= 2 else 0),
		"è«–ç†æ€§": 3 + (1 if logical_markers >= 3 else 0),
		"ç‹¬è‡ªæ€§": 3 + (1 if first_person else 0),
		"å®Ÿè·µæ€§": 3 + (1 if has_examples else 0),
		"è¡¨ç¾åŠ›": 3 + (1 if clarity and length >= 200 else 0),
	}
	
	# ã‚¹ã‚³ã‚¢ã‚’1ã€œ5ã®ç¯„å›²ã«èª¿æ•´
	for k in list(score_values.keys()):
		score_values[k] = max(1, min(5, score_values[k]))
	
	# ç†ç”±ã‚’ç”Ÿæˆï¼ˆã‚ˆã‚Šè©³ç´°ã§å…·ä½“çš„ãªç†ç”±ï¼‰
	reasons = {}
	
	# ç†è§£åº¦
	if logical_markers >= 3:
		reasons["ç†è§£åº¦"] = "è¬›ç¾©ã®é‡è¦æ¦‚å¿µï¼ˆãªãœã€ä»®èª¬ã€å‰æãªã©ï¼‰ã‚’é©åˆ‡ã«ç†è§£ã—ã€è«–ç†çš„ã«æ•´ç†ã•ã‚Œã¦ã„ã¾ã™"
	elif logical_markers >= 2:
		reasons["ç†è§£åº¦"] = "è¬›ç¾©å†…å®¹ã®åŸºæœ¬çš„ãªç†è§£ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚è«–ç†çš„ãªæ•´ç†ã‚’ã•ã‚‰ã«æ·±ã‚ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†"
	else:
		reasons["ç†è§£åº¦"] = "è¬›ç¾©å†…å®¹ã®ç†è§£ã‚’ç¤ºã™è¡¨ç¾ãŒã‚„ã‚„ä¸è¶³ã—ã¦ã„ã¾ã™ã€‚é‡è¦æ¦‚å¿µã‚’æ˜ç¢ºã«ã™ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†"
	
	# è«–ç†æ€§
	if logical_markers >= 3:
		reasons["è«–ç†æ€§"] = "è«–ç†çš„ãªã¤ãªãŒã‚Šï¼ˆã—ãŸãŒã£ã¦ã€ä¸€æ–¹ã§ã€ã¤ã¾ã‚Šãªã©ï¼‰ãŒæ˜ç¢ºã§ã€ä¸»å¼µã¨æ ¹æ‹ ãŒä¸€è²«ã—ã¦ã„ã¾ã™"
	elif logical_markers >= 2:
		reasons["è«–ç†æ€§"] = "è«–ç†çš„ãªã¤ãªãŒã‚ŠãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ä¸»å¼µã¨æ ¹æ‹ ã®é–¢ä¿‚ã‚’ã‚ˆã‚Šæ˜ç¢ºã«ã™ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†"
	else:
		reasons["è«–ç†æ€§"] = "è«–ç†çš„ãªã¤ãªãŒã‚Šã‚’ç¤ºã™è¡¨ç¾ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ä¸»å¼µã¨æ ¹æ‹ ã®é–¢ä¿‚ã‚’æ•´ç†ã™ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†"
	
	# ç‹¬è‡ªæ€§
	if first_person:
		reasons["ç‹¬è‡ªæ€§"] = "è‡ªèº«ã®çµŒé¨“ã‚„å®Ÿä½“é¨“ï¼ˆç§ã¯ã€è‡ªåˆ†ã€çµŒé¨“ãªã©ï¼‰ãŒæ˜ç¢ºã«ç¤ºã•ã‚Œã¦ãŠã‚Šã€ç‹¬è‡ªã®è¦–ç‚¹ãŒè¦‹ã‚‰ã‚Œã¾ã™"
	else:
		reasons["ç‹¬è‡ªæ€§"] = "è‡ªèº«ã®çµŒé¨“ã‚„è¦–ç‚¹ã‚’ç¤ºã™è¡¨ç¾ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚å®Ÿä½“é¨“ã‚„å…·ä½“ä¾‹ã‚’è¿½åŠ ã™ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†"
	
	# å®Ÿè·µæ€§
	if has_examples:
		reasons["å®Ÿè·µæ€§"] = "å…·ä½“çš„ãªäº‹ä¾‹ã‚„å®Ÿè·µçš„ãªå†…å®¹ï¼ˆå…·ä½“ã€äº‹ä¾‹ã€ç¾å ´ã€å®Ÿè£…ãªã©ï¼‰ãŒè±Šå¯Œã«å«ã¾ã‚Œã¦ãŠã‚Šã€å®Ÿè·µæ€§ãŒé«˜ã„ã§ã™"
	else:
		reasons["å®Ÿè·µæ€§"] = "å…·ä½“çš„ãªäº‹ä¾‹ã‚„å®Ÿè·µçš„ãªå†…å®¹ãŒã‚„ã‚„ä¸è¶³ã—ã¦ã„ã¾ã™ã€‚å®Ÿå‹™ã¸ã®å¿œç”¨ã‚’å…·ä½“çš„ã«ç¤ºã™ã¨è‰¯ã„ã§ã—ã‚‡ã†"
	
	# è¡¨ç¾åŠ›
	if clarity and length >= 500:
		reasons["è¡¨ç¾åŠ›"] = "æ–‡ç« ã®æ§‹é€ ãŒæ˜ç¢ºã§ã€ååˆ†ãªåˆ†é‡ãŒã‚ã‚Šã€ä¼ã‚ã‚Šã‚„ã™ã„è¡¨ç¾ã«ãªã£ã¦ã„ã¾ã™"
	elif clarity and length >= 200:
		reasons["è¡¨ç¾åŠ›"] = "æ–‡ç« ã®æ§‹é€ ã¯æ˜ç¢ºã§ã™ãŒã€ã‚‚ã†å°‘ã—è©³ã—ãå±•é–‹ã™ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†"
	else:
		reasons["è¡¨ç¾åŠ›"] = "æ–‡ç« ã®æ§‹é€ ã‚„æ˜å¿«ã•ã‚’å‘ä¸Šã•ã›ã‚‹ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚æ®µè½åˆ†ã‘ã‚„å…·ä½“ä¾‹ã‚’è¿½åŠ ã™ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†"
	
	# ã‚¹ã‚³ã‚¢ã¨ç†ç”±ã‚’çµåˆã—ãŸè¾æ›¸ã‚’è¿”ã™
	result = {}
	for category in ["ç†è§£åº¦", "è«–ç†æ€§", "ç‹¬è‡ªæ€§", "å®Ÿè·µæ€§", "è¡¨ç¾åŠ›"]:
		result[category] = {
			"score": score_values[category],
			"reason": reasons[category]
		}
	
	return result


# è¿½åŠ : åå°„ç”¨ãƒ‰ãƒ©ãƒ•ãƒˆç”Ÿæˆï¼ˆéLLMï¼‰
_DEF_NEXT_STEP = [
	("ä»®èª¬", "ä»®èª¬â†’æ¤œè¨¼ã®ãƒ«ãƒ¼ãƒ—ã‚’é€±æ¬¡ã§å›ã—ã€æ’¤é€€åŸºæº–ã‚‚ä¸€æ–‡ã§å®šç¾©ã—ã¾ã—ã‚‡ã†ã€‚"),
	("KPI", "å…ˆã«KPIã®å®šç¾©åŸŸã‚’åˆã‚ã›ã€å…¥åŠ›æŒ‡æ¨™ã¨å‡ºåŠ›æŒ‡æ¨™ã‚’åˆ†ã‘ã¦è­°è«–ã—ã¾ã—ã‚‡ã†ã€‚"),
	("é¡§å®¢", "é¡§å®¢ã®å…·ä½“çš„ãªè¡Œå‹•è¦³å¯Ÿã‚’ä¸€ã¤è¿½åŠ ã—ã€ä¾¡å€¤ä»®èª¬ã®ç¢ºåº¦ã‚’é«˜ã‚ã¾ã—ã‚‡ã†ã€‚"),
	("ä¾¡æ ¼", "ä¾¡æ ¼å—å®¹æ€§ã®ä»®èª¬ã‚’ç«‹ã¦ã€å°ã•ãªA/Bã§ä¸€æ¬¡æ¤œè¨¼ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"),
	("çµ„ç¹”", "æ„æ€æ±ºå®šã®è²¬ä»»å¢ƒç•Œã‚’æ˜ç¢ºã«ã—ã€å®Ÿè¡Œã®è©°ã¾ã‚Šã‚’å…ˆã«å¤–ã—ã¾ã—ã‚‡ã†ã€‚"),
]


def summarize_head(text: str, limit: int = 110) -> str:
	clean = re.sub(r"\s+", " ", text)
	clean = clean.replace("\u3000", " ")
	if len(clean) <= limit:
		return clean
	return clean[:limit].rstrip() + "â€¦"


def choose_next_step(text: str) -> str:
	for key, msg in _DEF_NEXT_STEP:
		if key in text:
			return msg
	return "æ¬¡å›ã¯ä»®èª¬ã®å‰æã‚’è¨€èªåŒ–ã—ã€å°ã•ãæ¤œè¨¼ã§ãã‚‹å˜ä½ã«åˆ†è§£ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"


def generate_reflection_draft(text: str, refs: List[str], scores: Dict[str, any]) -> str:
	lead = summarize_head(text)
	insight = "å“²å­¦ã¨ã—ã¦ã®ã€ã‚ã‚Šæ–¹ã€ã¨å®Ÿå‹™ã®å¾€å¾©ã‚’æ„è­˜ã§ãã¦ã„ã¾ã™ã€‚å‚ç…§ä¾‹ã‚’è¸ã¾ãˆã€æ„æ€æ±ºå®šã®è»¸ã‚’ä¸€æ–‡ã§ç½®ãã¾ã—ã‚‡ã†ã€‚"
	if refs:
		insight = "å‚ç…§ä¾‹ã«è¿‘ã„è«–ç‚¹ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚" + insight
	next_step = choose_next_step(text)
	return "\n".join([
		f"â—‹ {lead}",
		f"â—‹ {insight}",
		f"â—‹ {next_step}",
	])


# è¦ç´„æ©Ÿèƒ½: 3å½¢å¼ï¼ˆã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã€ç®‡æ¡æ›¸ãã€æ§‹é€ åŒ–ï¼‰
def call_openai_summary(prompt: str, system_prompt: str = None) -> Tuple[Optional[str], Optional[str]]:
	"""è¦ç´„ç”Ÿæˆç”¨ã®OpenAI APIå‘¼ã³å‡ºã—"""
	if not OPENAI_API_KEY:
		return None, "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
	try:
		messages = []
		if system_prompt:
			messages.append({"role": "system", "content": system_prompt})
		messages.append({"role": "user", "content": prompt})
		
		resp = requests.post(
			"https://api.openai.com/v1/chat/completions",
			headers={"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"},
			json={
				"model": LLM_MODEL,
				"messages": messages,
				"temperature": 0.3,  # è¦ç´„ã¯ä½ã„temperatureã§ä¸€è²«æ€§ã‚’ä¿ã¤
				"max_tokens": 800,
			},
			timeout=30,
		)
		resp.raise_for_status()
		data = resp.json()
		content = data.get("choices", [{}])[0].get("message", {}).get("content")
		return content, None
	except requests.exceptions.Timeout:
		return None, "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: APIæ¥ç¶šãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ"
	except requests.exceptions.RequestException as e:
		return None, f"APIã‚¨ãƒ©ãƒ¼: {str(e)}"
	except Exception as e:
		return None, f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}"


def generate_summary_llm(text: str) -> Tuple[Dict[str, any], bool]:
	"""LLMã‚’ä½¿ç”¨ã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã®è¦ç´„ã‚’è¦ç‚¹ã”ã¨ã«æ•´ç†ã—ãŸå½¢å¼ã§ç”Ÿæˆ
	Returns: (summary_dict, llm_actually_used)
	"""
	if not USE_LLM or not OPENAI_API_KEY:
		return generate_summary_fallback(text), False
	
	try:
		# è¦ç‚¹ã”ã¨ã«æ•´ç†ã•ã‚ŒãŸè¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
		summary_prompt = f"""ä»¥ä¸‹ã®å­¦ç”Ÿãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã‚“ã§ã€è¦ç‚¹ãŒã‚ã‹ã‚‹ã‚ˆã†ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ¬ãƒãƒ¼ãƒˆã€‘
{text}

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®å½¢å¼ã§è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„:

[ãƒ¬ãƒãƒ¼ãƒˆã®å…¨ä½“è¦ç´„ï¼š1-2æ®µè½ã§ã€ãƒ¬ãƒãƒ¼ãƒˆã®ç›®çš„ã€ä¸»è¦ãªãƒ†ãƒ¼ãƒã€è«–ç‚¹ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹ã€‚ç´„200-300æ–‡å­—ç¨‹åº¦ã€‚]

[ç•ªå·ä»˜ãã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ1ï¸âƒ£, 2ï¸âƒ£, 3ï¸âƒ£...ï¼‰ã§è¦ç‚¹ã‚’æ•´ç†ï¼šãƒ¬ãƒãƒ¼ãƒˆã®ä¸»è¦ãªè«–ç‚¹ã‚„è€ƒå¯Ÿã‚’3-5ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†ã‘ã¦èª¬æ˜ã™ã‚‹ã€‚
å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ä»¥ä¸‹ã®å½¢å¼ï¼š
1ï¸âƒ£ [ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—]

[ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†…å®¹ï¼š2-3æ–‡ã§èª¬æ˜ã€‚å¿…è¦ã«å¿œã˜ã¦ç®‡æ¡æ›¸ãã‚‚ä½¿ç”¨å¯ã€‚]

æœ€å¾Œã«ã€Œçµè«–ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ï¼š
[çµè«–]
[ãƒ¬ãƒãƒ¼ãƒˆã®çµè«–ã‚„ç·æ‹¬ï¼š1-2æ®µè½ã§ã¾ã¨ã‚ã‚‹ã€‚]

ã€æ³¨æ„äº‹é …ã€‘
- æ–‡ä½“ã¯ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§çµ±ä¸€ã—ã¦ãã ã•ã„
- é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ¦‚å¿µã¯**å¤ªå­—**ã§å¼·èª¿ã—ã¦ãã ã•ã„
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯èª­ã¿ã‚„ã™ãã€è¦ç‚¹ãŒæ˜ç¢ºã«ãªã‚‹ã‚ˆã†ã«æ§‹æˆã—ã¦ãã ã•ã„
- åŸæ–‡ã®é‡è¦ãªè«–ç‚¹ã‚„è€ƒå¯Ÿã‚’æ¼ã‚‰ã•ãšã€ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„
- æ–‡å­—æ•°ã¯åŸæ–‡ã®ç´„1/5ã‹ã‚‰1/8ç¨‹åº¦ã«åœ§ç¸®ã—ã¦ãã ã•ã„
- è¦‹å‡ºã—ã€ŒğŸ“˜è¦ç´„ã€ã¯å«ã‚ãªã„ã§ãã ã•ã„

è¦ç´„ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„:"""
		
		summary_text, error = call_openai_summary(
			summary_prompt,
			system_prompt="ã‚ãªãŸã¯å­¦è¡“çš„ãªè¦ç´„ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚æ–‡ç« ã®æœ¬è³ªã‚’æ‰ãˆã€è¦ç‚¹ãŒæ˜ç¢ºã§èª­ã¿ã‚„ã™ã„è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ãƒ¬ãƒãƒ¼ãƒˆã®ä¸»è¦ãªè«–ç‚¹ã‚„è€ƒå¯Ÿã‚’æ§‹é€ çš„ã«æ•´ç†ã—ã€èª­è€…ãŒçŸ­æ™‚é–“ã§å†…å®¹ã‚’ç†è§£ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚"
		)
		
		if not summary_text or error:
			return generate_summary_fallback(text), False
		
		# è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
		summary_clean = summary_text.strip()
		
		# ã€ŒğŸ“˜è¦ç´„ã€ã¨ã„ã†è¦‹å‡ºã—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤
		summary_clean = re.sub(r"^ğŸ“˜è¦ç´„\s*\n\s*\n?", "", summary_clean, flags=re.MULTILINE)
		summary_clean = re.sub(r"ğŸ“˜è¦ç´„\s*\n\s*\n?", "", summary_clean)
		summary_clean = summary_clean.strip()
		
		# æ—¢å­˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ã®äº’æ›æ€§ã®ãŸã‚ã€executiveãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚‚å…¨ä½“è¦ç´„ã‚’è¨­å®š
		# æœ€åˆã®æ®µè½ã‚’ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã¨ã—ã¦æŠ½å‡ºï¼ˆç•ªå·ä»˜ãã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å‰ã¾ã§ï¼‰
		executive_match = re.search(r"^(.*?)(?:\n\s*\n1ï¸âƒ£|$)", summary_clean, re.DOTALL)
		if executive_match:
			executive = executive_match.group(1).strip()
		else:
			# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®200æ–‡å­—
			executive = summary_clean[:200].strip()
		
		# ç®‡æ¡æ›¸ãè¦ç´„ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—ã‹ã‚‰æŠ½å‡º
		bullets = []
		section_matches = re.finditer(r"(\d+ï¸âƒ£)\s+([^\n]+)", summary_clean)
		for match in section_matches:
			bullets.append(match.group(2).strip())
		if len(bullets) > 5:
			bullets = bullets[:5]
		
		# æ§‹é€ åŒ–è¦ç´„ã¯ã€ä¸»è¦ãƒ†ãƒ¼ãƒã¨è¦ç‚¹ã‚’æŠ½å‡º
		structured = {
			"ä¸»è¦ãƒ†ãƒ¼ãƒ": executive[:50] if executive else "çµŒå–¶æˆ¦ç•¥ãƒ»å®Ÿè·µçš„è€ƒå¯Ÿ",
			"è¦ç‚¹": executive[:100] if executive else summarize_head(text, limit=100),
			"è€ƒå¯Ÿã®æ·±ã•": "ä¸­ç¨‹åº¦" if len(text) > 500 else "ç°¡æ½”",
			"å®Ÿè·µæ€§": "é«˜" if any(k in text for k in ["å…·ä½“", "äº‹ä¾‹", "ç¾å ´", "å®Ÿè£…"]) else "ä¸­",
		}
		
		# æ–°ã—ã„å½¢å¼ã®è¦ç´„ã‚’è¿”ã™ï¼ˆLLMä½¿ç”¨æˆåŠŸï¼‰
		return {
			"executive": summary_clean,  # å…¨ä½“ã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
			"bullets": bullets,
			"structured": structured,
			"formatted": summary_clean,  # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿è¦ç´„
		}, True  # LLMãŒæ­£å¸¸ã«ä½¿ç”¨ã•ã‚ŒãŸ
	except Exception as e:
		# ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«æˆ»ã‚‹
		return generate_summary_fallback(text), False


def generate_summary_fallback(text: str) -> Dict[str, any]:
	"""ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“çš„ãªè¦ç´„ç”Ÿæˆï¼ˆLLMä¸ä½¿ç”¨ï¼‰"""
	clean_text = re.sub(r"\s+", " ", text.strip())
	
	# ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰
	executive = summarize_head(text, limit=200)
	if len(clean_text) > 200:
		executive += " æœ¬ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ã€å…·ä½“çš„ãªäº‹ä¾‹ã‚„è€ƒå¯Ÿã‚’é€šã˜ã¦ã€å®Ÿè·µçš„ãªè¦–ç‚¹ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚"
	
	# ç®‡æ¡æ›¸ãè¦ç´„ï¼ˆä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºï¼‰
	bullets = []
	sentences = re.split(r"[ã€‚\n]", clean_text)
	key_sentences = [s.strip() for s in sentences if len(s.strip()) > 20][:5]
	for i, sent in enumerate(key_sentences[:5], 1):
		if sent:
			bullets.append(f"{sent}ã€‚")
	
	# æ§‹é€ åŒ–è¦ç´„
	structured = {
		"ä¸»è¦ãƒ†ãƒ¼ãƒ": "çµŒå–¶æˆ¦ç•¥ãƒ»å®Ÿè·µçš„è€ƒå¯Ÿ",
		"è¦ç‚¹": summarize_head(text, limit=100),
		"è€ƒå¯Ÿã®æ·±ã•": "ä¸­ç¨‹åº¦" if len(text) > 500 else "ç°¡æ½”",
		"å®Ÿè·µæ€§": "é«˜" if any(k in text for k in ["å…·ä½“", "äº‹ä¾‹", "ç¾å ´", "å®Ÿè£…"]) else "ä¸­",
	}
	
	# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚æ–°ã—ã„å½¢å¼ã«åˆã‚ã›ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰
	summary_formatted = f"""{executive}

1ï¸âƒ£ ä¸»è¦ãªè«–ç‚¹

{bullets[0] if bullets else "æœ¬ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ã€çµŒå–¶æˆ¦ç•¥ã«é–¢ã™ã‚‹è€ƒå¯ŸãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚"}

2ï¸âƒ£ å®Ÿè·µçš„ãªè¦–ç‚¹

æœ¬ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ã€å…·ä½“çš„ãªäº‹ä¾‹ã‚„è€ƒå¯Ÿã‚’é€šã˜ã¦ã€å®Ÿè·µçš„ãªè¦–ç‚¹ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

[çµè«–]

{executive}
"""
	
	return {
		"executive": summary_formatted,  # æ–°ã—ã„å½¢å¼ã«å¯¾å¿œ
		"bullets": bullets,
		"structured": structured,
		"formatted": summary_formatted,
	}


def generate_summary(text: str) -> Tuple[Dict[str, any], bool, Optional[str]]:
	"""ãƒ¬ãƒãƒ¼ãƒˆã®è¦ç´„ã‚’3å½¢å¼ã§ç”Ÿæˆï¼ˆå¸¸ã«LLMä½¿ç”¨ï¼‰
	Returns: (summary_dict, llm_used_for_summary, error_message)
	"""
	if not OPENAI_API_KEY:
		error_msg = "è¦ç´„ç”Ÿæˆã«ã¯OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
		return {
			"executive": error_msg,
			"bullets": [],
			"structured": {},
			"formatted": error_msg,
		}, False, error_msg
	
	if not USE_LLM:
		error_msg = "LLMãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚ç’°å¢ƒå¤‰æ•°USE_LLM=1ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
		return {
			"executive": error_msg,
			"bullets": [],
			"structured": {},
			"formatted": error_msg,
		}, False, error_msg
	
	try:
		summary_result, llm_used = generate_summary_llm(text)
		return summary_result, llm_used, None
	except Exception as e:
		error_msg = f"è¦ç´„ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
		return {
			"executive": error_msg,
			"bullets": [],
			"structured": {},
			"formatted": error_msg,
		}, False, error_msg


# LLMé€£æºï¼ˆOpenAI APIï¼‰: ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
USE_LLM = os.environ.get("USE_LLM", "0") in ("1", "true", "TRUE", "on")
LLM_MODEL = os.environ.get("LLM_MODEL", "gpt-4o-mini")


def build_llm_prompt(text: str, doc_type: str, refs: List[str], scores: Dict[str, any]) -> str:
	base = load_prompt("reflection" if doc_type == "reflection" else "final")
	
	# å‚ç…§ä¾‹ã®æ–‡å­—æ•°ã‚’åˆ†æã—ã¦ã€å¹³å‡æ–‡å­—æ•°ã‚’è¨ˆç®—
	ref_lengths = []
	refs_text_list = []
	ref_details = []  # å„å‚ç…§ä¾‹ã®è©³ç´°æƒ…å ±ï¼ˆæ–‡å­—æ•°ã‚’å«ã‚€ï¼‰
	
	for i, ref in enumerate(refs, 1):
		if ref:
			# JSONã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸæ”¹è¡Œæ–‡å­—ï¼ˆ\\nï¼‰ã‚’å®Ÿéš›ã®æ”¹è¡Œã«å¤‰æ›
			ref_normalized = ref.replace("\\n", "\n")
			# æ”¹è¡Œã¨ç©ºç™½ã‚’é™¤ã„ãŸå®Ÿéš›ã®æ–‡å­—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæ—¥æœ¬èªæ–‡å­—æ•°ã‚’æ­£ç¢ºã«æ¸¬å®šï¼‰
			ref_clean = ref_normalized.replace("\n", "").replace(" ", "").replace("\t", "").replace("\\n", "")
			ref_length = len(ref_clean)
			ref_lengths.append(ref_length)
			refs_text_list.append(f"- {ref_normalized}")
			ref_details.append(f"å‚ç…§ä¾‹{i}: {ref_length}æ–‡å­—")
	
	# å‚ç…§ä¾‹ã®å¹³å‡æ–‡å­—æ•°ã‚’è¨ˆç®—ï¼ˆå‚ç…§ä¾‹ãŒã‚ã‚‹å ´åˆï¼‰
	target_length = None
	length_instruction = ""
	avg_length = None
	if ref_lengths:
		avg_length = sum(ref_lengths) / len(ref_lengths)
		# å¹³å‡æ–‡å­—æ•°ã®Â±10%ã®ç¯„å›²ã‚’ç›®æ¨™æ–‡å­—æ•°ã¨ã™ã‚‹ï¼ˆã‚ˆã‚Šå³å¯†ã«ï¼‰
		min_length = max(200, int(avg_length * 0.9))  # æœ€å°200æ–‡å­—
		max_length = int(avg_length * 1.1)
		target_length = f"{min_length}ã€œ{max_length}æ–‡å­—"
		
		# å„å‚ç…§ä¾‹ã®æ–‡å­—æ•°ã‚’æ˜ç¤º
		ref_lengths_text = "ã€".join(ref_details)
		length_instruction = f"""
ã€é‡è¦ï¼šæ–‡å­—æ•°æŒ‡å®šï¼ˆå³å®ˆï¼‰ã€‘
å‚ç…§ä¾‹ã®æ–‡å­—æ•°: {ref_lengths_text}
å‚ç…§ä¾‹ã®å¹³å‡æ–‡å­—æ•°: ç´„{int(avg_length)}æ–‡å­—
ç›®æ¨™æ–‡å­—æ•°: {target_length}ï¼ˆå¹³å‡ã®Â±10%ä»¥å†…ï¼‰

âš ï¸ çµ¶å¯¾ã«å®ˆã£ã¦ãã ã•ã„ï¼š
- å¿…ãšå‚ç…§ä¾‹ã¨åŒã˜ãã‚‰ã„ã®åˆ†é‡ã§ç”Ÿæˆã—ã¦ãã ã•ã„
- å‚ç…§ä¾‹ã®æ–‡å­—æ•°ã‚’å‚è€ƒã«ã€{int(avg_length)}æ–‡å­—å‰å¾Œï¼ˆ{target_length}ã®ç¯„å›²å†…ï¼‰ã§ç”Ÿæˆã™ã‚‹ã“ã¨ãŒæœ€é‡è¦ã§ã™
- å‚ç…§ä¾‹ã¨åŒã˜æ–‡ä½“ãƒ»åŒã˜åˆ†é‡æ„Ÿãƒ»åŒã˜è©³ç´°åº¦ã§ç”Ÿæˆã—ã¦ãã ã•ã„
- çŸ­ã™ãã¦ã‚‚é•·ã™ãã¦ã‚‚ã„ã‘ã¾ã›ã‚“ã€‚å‚ç…§ä¾‹ã®å¹³å‡æ–‡å­—æ•°ï¼ˆ{int(avg_length)}æ–‡å­—ï¼‰ã«ã§ãã‚‹ã ã‘è¿‘ã„åˆ†é‡ã§ç”Ÿæˆã—ã¦ãã ã•ã„
"""
	
	refs_text = "\n".join(refs_text_list) if refs_text_list else "(å‚ç…§ãªã—)"
	
	# scoresã®å½¢å¼ã‚’ç¢ºèªï¼ˆæ–°ã—ã„å½¢å¼: {"score": int, "reason": str} ã¾ãŸã¯ å¤ã„å½¢å¼: intï¼‰
	scores_list = []
	for category in ["ç†è§£åº¦", "è«–ç†æ€§", "ç‹¬è‡ªæ€§", "å®Ÿè·µæ€§", "è¡¨ç¾åŠ›"]:
		value = scores.get(category, {})
		if isinstance(value, dict):
			score_val = value.get("score", 0)
			scores_list.append(f"{category}:{score_val}")
		else:
			scores_list.append(f"{category}:{value}")
	scores_text = ", ".join(scores_list) if scores_list else ""
	
	# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ–‡å­—æ•°æŒ‡å®šéƒ¨åˆ†ã‚’å‹•çš„ã«ç½®æ›
	if target_length:
		# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ã€Œæ–‡å­—æ•°: 300ã€œ400æ–‡å­—ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç½®æ›
		# ã‚ˆã‚ŠæŸ”è»Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå…¨è§’ãƒ»åŠè§’ã‚³ãƒ­ãƒ³ã€æ³¢ç·šãƒ»ãƒã‚¤ãƒ•ãƒ³ã«å¯¾å¿œï¼‰
		base = re.sub(r"æ–‡å­—æ•°[ï¼š:]\s*å‚ç…§ä¾‹.*?æ–‡å­—", f"æ–‡å­—æ•°: {target_length}", base, flags=re.DOTALL)
		base = re.sub(r"æ–‡å­—æ•°[ï¼š:]\s*\d+[ã€œ~-]\d+æ–‡å­—[ï¼ˆ(].*?[ï¼‰)]?", f"æ–‡å­—æ•°: {target_length}ï¼ˆå‚ç…§ä¾‹ã®åˆ†é‡ã«åˆã‚ã›ã‚‹ï¼‰", base)
		base = re.sub(r"æ–‡å­—æ•°[ï¼š:]\s*\d+[ã€œ~-]\d+æ–‡å­—", f"æ–‡å­—æ•°: {target_length}ï¼ˆå‚ç…§ä¾‹ã®åˆ†é‡ã«åˆã‚ã›ã‚‹ï¼‰", base)
		base = re.sub(r"- æ–‡å­—æ•°[ï¼š:]\s*\d+[ã€œ~-]\d+æ–‡å­—", f"- æ–‡å­—æ•°: {target_length}ï¼ˆå‚ç…§ä¾‹ã®åˆ†é‡ã«åˆã‚ã›ã‚‹ï¼‰", base)
	
	return (
		f"{base}\n\nã€å‚ç…§ä¾‹ï¼ˆæ–‡ä½“ã®ãƒ’ãƒ³ãƒˆãƒ»åˆ†é‡ã®ç›®å®‰ï¼‰ã€‘\n{refs_text}{length_instruction}\n"
		f"ã€Rubricæ‰€æ„Ÿï¼ˆç›®å®‰ï¼‰ã€‘\n{scores_text}\n\n"
		f"ã€å…¥åŠ›ãƒ¬ãƒãƒ¼ãƒˆã€‘\n{text}\n"
	)


def call_openai(prompt: str, max_tokens: int = 500, system_message: str = None) -> Tuple[Optional[str], Optional[str]]:
	"""OpenAI APIã‚’å‘¼ã³å‡ºã—ã€çµæœã¨ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
	Args:
		prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
		max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ500ã€å‚ç…§ä¾‹ã«å¿œã˜ã¦å‹•çš„ã«èª¿æ•´ï¼‰
		system_message: ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ¨™æº–ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
	"""
	if not OPENAI_API_KEY:
		return None, "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
	
	if system_message is None:
		system_message = "ã‚ãªãŸã¯çµŒå–¶æˆ¦ç•¥è«–ã®æ•™æˆã§ã™ã€‚æ•¬æ„ã¨æ¸©ã‹ã•ã‚’ä¿ã¡ã€1ã¤ã®ã¾ã¨ã¾ã£ãŸæ–‡ç« ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
	
	try:
		resp = requests.post(
			"https://api.openai.com/v1/chat/completions",
			headers={"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"},
			json={
				"model": LLM_MODEL,
				"messages": [
					{"role": "system", "content": system_message},
					{"role": "user", "content": prompt},
				],
				"temperature": 0.6,
				"max_tokens": max_tokens,
			},
			timeout=30,
		)
		resp.raise_for_status()
		data = resp.json()
		content = data.get("choices", [{}])[0].get("message", {}).get("content")
		return content, None
	except requests.exceptions.Timeout:
		return None, "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: APIæ¥ç¶šãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ"
	except requests.exceptions.RequestException as e:
		return None, f"APIã‚¨ãƒ©ãƒ¼: {str(e)}"
	except Exception as e:
		return None, f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}"


@app.post("/generate", response_model=GenerateResponse)
async def generate(req: GenerateRequest) -> GenerateResponse:
	return build_comment(req)


# ç›´æ¥ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/generate_direct")
async def generate_direct(req: DirectGenRequest):
	doc_type = (req.type or "reflection")
	refs = retrieve_refs(req.text, doc_type, k=2)
	scores = simple_score(req.text)
	llm_used = False
	llm_error = None
	draft = None
	
	# å‚ç…§ä¾‹ã®å¹³å‡æ–‡å­—æ•°ã‚’è¨ˆç®—ã—ã¦ã€max_tokensã‚’å‹•çš„ã«è¨­å®š
	avg_length = None
	max_tokens = 500  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
	system_message = "ã‚ãªãŸã¯çµŒå–¶æˆ¦ç•¥è«–ã®æ•™æˆã§ã™ã€‚æ•¬æ„ã¨æ¸©ã‹ã•ã‚’ä¿ã¡ã€1ã¤ã®ã¾ã¨ã¾ã£ãŸæ–‡ç« ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
	
	if refs:
		ref_lengths = []
		for ref in refs:
			if ref:
				ref_normalized = ref.replace("\\n", "\n")
				ref_clean = ref_normalized.replace("\n", "").replace(" ", "").replace("\t", "").replace("\\n", "")
				ref_length = len(ref_clean)
				ref_lengths.append(ref_length)
		
		if ref_lengths:
			avg_length = sum(ref_lengths) / len(ref_lengths)
			# æ—¥æœ¬èªã®å ´åˆã€1æ–‡å­—â‰ˆ2ãƒˆãƒ¼ã‚¯ãƒ³ç¨‹åº¦ã€‚å®‰å…¨ã®ãŸã‚å¹³å‡æ–‡å­—æ•°ã®2.5å€ã‚’è¨­å®š
			# æœ€å°500ã€æœ€å¤§2000ã«åˆ¶é™
			calculated_tokens = int(avg_length * 2.5)
			max_tokens = max(500, min(calculated_tokens, 2000))
			
			# system_messageã«æ–‡å­—æ•°ã«é–¢ã™ã‚‹æŒ‡ç¤ºã‚’è¿½åŠ 
			system_message = f"""ã‚ãªãŸã¯çµŒå–¶æˆ¦ç•¥è«–ã®æ•™æˆã§ã™ã€‚æ•¬æ„ã¨æ¸©ã‹ã•ã‚’ä¿ã¡ã€1ã¤ã®ã¾ã¨ã¾ã£ãŸæ–‡ç« ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

é‡è¦ï¼šå‚ç…§ä¾‹ã®å¹³å‡æ–‡å­—æ•°ã¯ç´„{int(avg_length)}æ–‡å­—ã§ã™ã€‚å¿…ãšã“ã®åˆ†é‡ã«åˆã‚ã›ã¦ã€åŒã˜ãã‚‰ã„ã®æ–‡é‡ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"""
	
	if USE_LLM and os.environ.get("OPENAI_API_KEY"):
		prompt = build_llm_prompt(req.text, doc_type, refs, scores)
		draft, llm_error = call_openai(prompt, max_tokens=max_tokens, system_message=system_message)
		llm_used = draft is not None and llm_error is None
	if not draft:
		if doc_type == "reflection":
			draft = generate_reflection_draft(req.text, refs, scores)
		else:
			draft = "\n".join([
				"å…¨ä½“è©•ä¾¡: å­¦ã³ã®æ¥ç¶šã¨ä»®èª¬ã®ç­‹ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚",
				"å¼·ã¿: ç¾å ´è¦³å¯Ÿã®å…·ä½“æ€§ã€‚",
				"æ”¹å–„: æŒ‡æ¨™ãƒ»æ’¤é€€åŸºæº–ã®æ˜ç¢ºåŒ–ã€‚",
				"ç·æ‹¬: ã‚ã‚Šæ–¹ã«ç«‹è„šã—ã€æ¬¡ã®ä¸€æ­©ã‚’å…·ä½“åŒ–ã—ã¾ã—ã‚‡ã†ã€‚",
			])
	
	# è¦ç´„ç”Ÿæˆï¼ˆå¸¸ã«LLMä½¿ç”¨ï¼‰
	summary, summary_llm_used, summary_error = generate_summary(req.text)
	
	return {
		"report_id": None,
		"feedback_id": None,
		"ai_comment": draft,
		"rubric": scores,
		"summary": summary,
		"used_refs": refs,
		"llm_used": llm_used,
		"llm_error": llm_error,
		"summary_llm_used": summary_llm_used,  # è¦ç´„ç”Ÿæˆã§LLMãŒä½¿ã‚ã‚ŒãŸã‹
		"summary_error": summary_error,  # è¦ç´„ç”Ÿæˆã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚ã‚Œã°ï¼‰
	}


@app.get("/health")
async def health():
	return {"ok": True}
