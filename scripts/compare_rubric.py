#!/usr/bin/env python3
"""
ç”ŸæˆRubricã¨äººé–“è©•ä¾¡ã®å·®åˆ†ã‚’åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ã„æ–¹:
    python scripts/compare_rubric.py --generated data/eval/generated.csv --human data/eval/human_scores.csv
"""

import argparse
import csv
import json
import pathlib
import sys
from statistics import mean, stdev
from typing import Dict, List

ROOT = pathlib.Path(__file__).resolve().parents[1]

RUBRIC_CATEGORIES = ["ç†è§£åº¦", "è«–ç†æ€§", "ç‹¬è‡ªæ€§", "å®Ÿè·µæ€§", "è¡¨ç¾åŠ›"]


def load_generated_csv(csv_path: pathlib.Path) -> Dict[str, Dict]:
    """ç”ŸæˆçµæœCSVã‚’èª­ã¿è¾¼ã¿"""
    results = {}
    
    with open(csv_path, encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            report_name = row.get("report_name", row.get("report_file", "")).replace(".txt", "")
            
            rubric = {}
            for cat in RUBRIC_CATEGORIES:
                key = f"rubric_{cat}"
                try:
                    rubric[cat] = int(row.get(key, 0))
                except (ValueError, TypeError):
                    rubric[cat] = 0
            
            results[report_name] = {
                "rubric": rubric,
                "rubric_total": sum(rubric.values()),
                "ai_comment": row.get("ai_comment", ""),
                "llm_used": row.get("llm_used", "False").lower() == "true",
                "prompt_version": row.get("prompt_version", ""),
                "model_version": row.get("model_version", ""),
            }
    
    return results


def load_human_scores_csv(csv_path: pathlib.Path) -> Dict[str, Dict]:
    """äººé–“è©•ä¾¡CSVã‚’èª­ã¿è¾¼ã¿"""
    results = {}
    
    with open(csv_path, encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            report_id = row.get("è©•ä¾¡å¯¾è±¡ID", "").strip()
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®åˆ—åã«å¯¾å¿œ
            rubric = {}
            for cat in RUBRIC_CATEGORIES:
                # åˆ—åã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«å¯¾å¿œ
                key_candidates = [
                    f"AI_{cat}",
                    f"ç”Ÿæˆ{cat}",
                    cat,
                    f"rubric_{cat}",
                ]
                value = None
                for key in key_candidates:
                    if key in row and row[key].strip():
                        try:
                            value = int(row[key])
                            break
                        except (ValueError, TypeError):
                            continue
                rubric[cat] = value if value is not None else 0
            
            # æ‰‹å‹•è©•ä¾¡ï¼ˆæ•™æˆã®ä¿®æ­£å¾ŒRubricï¼‰ã‚‚èª­ã¿è¾¼ã¿
            human_rubric = {}
            for cat in RUBRIC_CATEGORIES:
                key_candidates = [
                    f"æ•™æˆ_{cat}",
                    f"æ‰‹å‹•{cat}",
                    f"human_{cat}",
                ]
                value = None
                for key in key_candidates:
                    if key in row and row[key].strip():
                        try:
                            value = int(row[key])
                            break
                        except (ValueError, TypeError):
                            continue
                human_rubric[cat] = value if value is not None else None
            
            results[report_id] = {
                "rubric": rubric,
                "human_rubric": human_rubric if any(v is not None for v in human_rubric.values()) else None,
                "theme": row.get("ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ¼ãƒ", ""),
            }
    
    return results


def calculate_diff(ai_rubric: Dict[str, int], human_rubric: Dict[str, int]) -> Dict[str, float]:
    """å·®åˆ†ã‚’è¨ˆç®—"""
    diffs = {}
    for cat in RUBRIC_CATEGORIES:
        ai_val = ai_rubric.get(cat, 0)
        human_val = human_rubric.get(cat, 0)
        diffs[cat] = abs(ai_val - human_val)
    return diffs


def analyze_comparison(generated: Dict[str, Dict], human: Dict[str, Dict]):
    """æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ"""
    
    # ãƒãƒƒãƒãƒ³ã‚°ï¼ˆreport_nameã¨è©•ä¾¡å¯¾è±¡IDã®å¯¾å¿œï¼‰
    matches: List[Dict] = []
    
    for report_id, human_data in human.items():
        # report_idã‹ã‚‰report_nameã‚’æ¨æ¸¬ï¼ˆtest_001 -> test_001 ã¾ãŸã¯ 001ï¼‰
        candidates = [report_id, report_id.replace("test_", ""), report_id.replace("_", "")]
        
        matched_data = None
        for candidate in candidates:
            if candidate in generated:
                matched_data = generated[candidate]
                break
        
        if matched_data is None:
            # éƒ¨åˆ†ãƒãƒƒãƒã‚’è©¦ã™
            for gen_name in generated.keys():
                if report_id in gen_name or gen_name in report_id:
                    matched_data = generated[gen_name]
                    break
        
        if matched_data is None:
            print(f"âš ï¸  ãƒãƒƒãƒãƒ³ã‚°å¤±æ•—: {report_id}", file=sys.stderr)
            continue
        
        ai_rubric = matched_data["rubric"]
        human_rubric = human_data.get("human_rubric") or human_data["rubric"]
        
        diffs = calculate_diff(ai_rubric, human_rubric)
        
        matches.append({
            "report_id": report_id,
            "theme": human_data.get("theme", ""),
            "ai_rubric": ai_rubric,
            "human_rubric": human_rubric,
            "diffs": diffs,
            "ai_total": sum(ai_rubric.values()),
            "human_total": sum(human_rubric.values()),
            "total_diff": abs(sum(ai_rubric.values()) - sum(human_rubric.values())),
        })
    
    if not matches:
        print("âŒ ãƒãƒƒãƒãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    # é›†è¨ˆ
    print("\n" + "=" * 70)
    print("ğŸ“Š Rubricåˆè‡´ç‡åˆ†æ")
    print("=" * 70)
    print(f"\nè©•ä¾¡ä»¶æ•°: {len(matches)}ä»¶\n")
    
    # è¦³ç‚¹åˆ¥å¹³å‡å·®åˆ†
    avg_diffs = {}
    for cat in RUBRIC_CATEGORIES:
        cat_diffs = [m["diffs"][cat] for m in matches]
        avg_diffs[cat] = {
            "mean": mean(cat_diffs),
            "max": max(cat_diffs),
            "stdev": stdev(cat_diffs) if len(cat_diffs) > 1 else 0,
        }
    
    print("ã€è¦³ç‚¹åˆ¥å¹³å‡å·®åˆ†ã€‘")
    print(f"{'è¦³ç‚¹':<10} {'å¹³å‡å·®åˆ†':<10} {'æœ€å¤§å·®åˆ†':<10} {'æ¨™æº–åå·®':<10} {'åˆ¤å®š'}")
    print("-" * 70)
    
    for cat in RUBRIC_CATEGORIES:
        d = avg_diffs[cat]
        target = 0.5  # ç›®æ¨™: Â±0.5ä»¥å†…
        status = "âœ…" if d["mean"] <= target else "âš ï¸" if d["mean"] <= 1.0 else "âŒ"
        print(f"{cat:<10} {d['mean']:<10.2f} {d['max']:<10} {d['stdev']:<10.2f} {status}")
    
    # å…¨ä½“å¹³å‡å·®åˆ†
    overall_avg_diff = mean([m["total_diff"] / len(RUBRIC_CATEGORIES) for m in matches])
    overall_max_diff = max([m["total_diff"] / len(RUBRIC_CATEGORIES) for m in matches])
    
    print(f"\nã€å…¨ä½“å¹³å‡å·®åˆ†ã€‘")
    print(f"  å¹³å‡: {overall_avg_diff:.2f}ç‚¹ï¼ˆç›®æ¨™: â‰¤0.5ç‚¹ï¼‰")
    print(f"  æœ€å¤§: {overall_max_diff:.2f}ç‚¹")
    
    if overall_avg_diff <= 0.5:
        print(f"  åˆ¤å®š: âœ… åˆæ ¼ï¼ˆç›®æ¨™é”æˆï¼‰")
    elif overall_avg_diff <= 1.0:
        print(f"  åˆ¤å®š: âš ï¸  è¦æ”¹å–„ï¼ˆç›®æ¨™ã«è¿‘ã„ãŒæœªé”ï¼‰")
    else:
        print(f"  åˆ¤å®š: âŒ è¦å†èª¿æ•´ï¼ˆç›®æ¨™ã¨ä¹–é›¢ï¼‰")
    
    # å€‹åˆ¥çµæœ
    print(f"\nã€å€‹åˆ¥çµæœã€‘")
    print(f"{'ID':<15} {'AIåˆè¨ˆ':<10} {'æ•™æˆåˆè¨ˆ':<10} {'å·®åˆ†':<10} {'åˆ¤å®š'}")
    print("-" * 70)
    
    for m in sorted(matches, key=lambda x: x["total_diff"], reverse=True):
        status = "âœ…" if m["total_diff"] / len(RUBRIC_CATEGORIES) <= 0.5 else "âš ï¸" if m["total_diff"] / len(RUBRIC_CATEGORIES) <= 1.0 else "âŒ"
        print(f"{m['report_id']:<15} {m['ai_total']:<10} {m['human_total']:<10} {m['total_diff']:<10} {status}")
        print(f"  AI: {dict(m['ai_rubric'])}")
        print(f"  æ•™æˆ: {dict(m['human_rubric'])}")
        if m["theme"]:
            print(f"  ãƒ†ãƒ¼ãƒ: {m['theme']}")
        print()
    
    # æ”¹å–„ãŒå¿…è¦ãªè¦³ç‚¹
    print("ã€æ”¹å–„ãŒå¿…è¦ãªè¦³ç‚¹ã€‘")
    for cat in RUBRIC_CATEGORIES:
        d = avg_diffs[cat]
        if d["mean"] > 0.5:
            print(f"  âŒ {cat}: å¹³å‡å·®åˆ† {d['mean']:.2f}ç‚¹ï¼ˆç›®æ¨™: â‰¤0.5ç‚¹ï¼‰")
        elif d["mean"] > 0.3:
            print(f"  âš ï¸  {cat}: å¹³å‡å·®åˆ† {d['mean']:.2f}ç‚¹ï¼ˆè¦ç›£è¦–ï¼‰")
        else:
            print(f"  âœ… {cat}: å¹³å‡å·®åˆ† {d['mean']:.2f}ç‚¹ï¼ˆå•é¡Œãªã—ï¼‰")


def main():
    parser = argparse.ArgumentParser(description="ç”ŸæˆRubricã¨äººé–“è©•ä¾¡ã®å·®åˆ†åˆ†æ")
    parser.add_argument(
        "--generated",
        type=pathlib.Path,
        required=True,
        help="ç”ŸæˆçµæœCSV (ä¾‹: data/eval/generated_YYYYMMDD_HHMMSS.csv)"
    )
    parser.add_argument(
        "--human",
        type=pathlib.Path,
        required=True,
        help="äººé–“è©•ä¾¡CSV (ä¾‹: data/eval/human_scores.csv)"
    )
    
    args = parser.parse_args()
    
    generated_path = args.generated
    if not generated_path.is_absolute():
        generated_path = ROOT / generated_path
    
    human_path = args.human
    if not human_path.is_absolute():
        human_path = ROOT / human_path
    
    if not generated_path.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {generated_path}")
        sys.exit(1)
    
    if not human_path.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {human_path}")
        sys.exit(1)
    
    print(f"ğŸ“„ ç”Ÿæˆçµæœ: {generated_path}")
    print(f"ğŸ“„ äººé–“è©•ä¾¡: {human_path}")
    
    generated_data = load_generated_csv(generated_path)
    human_data = load_human_scores_csv(human_path)
    
    analyze_comparison(generated_data, human_data)


if __name__ == "__main__":
    main()



